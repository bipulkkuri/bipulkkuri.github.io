<!DOCTYPE html>
<html lang="en-US">

<head>
  <meta charset='utf-8'>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <link rel="stylesheet" href="/assets/css/style.css?v=74117ed9f08b56b9f421224c61d91e4c8b0da4e5" media="screen"
    type="text/css">
  <link rel="stylesheet" href="/assets/css/print.css" media="print" type="text/css">

  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->

  <!-- Begin Jekyll SEO tag v2.8.0 -->
  <title>Bipul</title>
  <meta name="generator" content="Jekyll v3.10.0" />
  <meta property="og:title" content="Bipul" />
  <meta property="og:locale" content="en_US" />
  <link rel="canonical" href="https://bipulkkuri.github.io/" />
  <meta property="og:url" content="https://bipulkkuri.github.io/" />
  <meta property="og:site_name" content="Bipul" />
  <meta property="og:type" content="website" />
  <meta name="twitter:card" content="summary" />
  <meta property="twitter:title" content="Bipul" />
  <script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","headline":"Bipul","name":"Bipul","url":"https://bipulkkuri.github.io/"}</script>
  <!-- End Jekyll SEO tag -->


  <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

  <!-- Setup Google Analytics -->



  <!-- You can set your favicon here -->
  <!-- link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" -->

  <!-- end custom head snippets -->

</head>

<body>
  <header>
    <div class="inner">
      <a href="https://bipulkkuri.github.io/">
        <h1>AWS Data Engineering Associate Certification _ WIP</h1>
      </a>
      <h2></h2>


      <a href="https://github.com/bipulkkuri" class="button"><small>Follow me on</small> GitHub</a>

    </div>
  </header>

  <div id="content-wrapper">
    <div class="inner clearfix">
      <section id="main-content">

        <p>This page gives overview of AWS services required for AWS Data Engineering Associate certification.Consider
          this as a cheat sheet,<br><b>A visual representation of things to cover.</b></p>

        <p><img src="/docs/assets/images/AWS-DEA-C01-Layout.png" alt="AWS DEA-C01" /></p>
        <p><br><b>Data Warehouse vs Data Lake vs Data LakeHouse</b><br></p>

        <table style="width: 100%;" border="solid black" border-radius="10px;">
          <tbody>
            <tr>
              <td><b>Data Warehouse</b></td>
              <td><b>Data Lake</b></td>
              <td><b>Data LakeHouse</b></td>
            </tr>
            <tr>
              <td>costly</td>
              <td>Often more cost-effective</td>
              <td>Often more cost-effective</td>
            </tr>
            <tr>
              <td>Access is typically limited to business analysts and decision-makers who require structured data for
                reporting.</td>
              <td>Accessible to a broader range of users, including data scientists and engineers, who can work with raw
                data for various analytical purposes.</td>
              <td>mix use</td>
            </tr>
            <tr>
              <td>ETL</td>
              <td>ELT</td>
              <td>&nbsp;</td>
            </tr>
            <tr>
              <td>Primarily stores structured data. Data is organized into tables with defined schemas.</td>
              <td>Can store structured, semi-structured, and unstructured data. Data is kept in its raw form.</td>
              <td>Can store structured, semi-structured, and unstructured data. Data is kept in its raw form.</td>
            </tr>
            <tr>
              <td>Designed for reporting and analysis. It supports business intelligence tools and provides insights
                through predefined queries</td>
              <td>Intended for data exploration and experimentation. It allows for advanced analytics, machine learning,
                and big data processing</td>
              <td>ACID transactions</td>
            </tr>
            <tr>
              <td>Example: RedShift</td>
              <td>Example: S3 </td>
              <td>Example: AWS Lake formation (s3 as lake , redhift spectrum as warehouse) </td>
            </tr>
            <tr>
              <td>Use when: you have structred data ,fast complex queries,data integration from different sources,BI and
                analytics need,historical data analysis</td>
              <td>Use when: mixture of data , when you dont know future use as its flexible, Advanced analytics, ML ,
                data discovery goals</td>
              <td>mix use cost effective</td>
            </tr>
          </tbody>
        </table>
        <p><br><b>Use case by AWS Resources</b> <br></p>
        <table style="width: 100%;" border="solid black" border-radius="10px;">
          <tbody>
            <tr>
              <td><b>Component</b></td>
              <td><b>Usecase</b></td>
              <td><b>Cost</b></td>
              <td><b>Availability</b></td>
            </tr>
            <tr>
              <td>AWS S3</td>
              <td>Object Storage: unlimited data storage, backup and recovery,
                and big data analytics,web hosting,backup recovery,application data
                when data is colder and large,BLOB
              </td>
              <td>cheap and based on tier</td>
              <td>HA</td>
            </tr>
            <tr>
              <td>EFS</td>
              <td>Content Management ,web serving, data sharing ,wordpress</td>
              <td>Costly</td>
              <td>HA, One EFS can connect to multiple instances across Zones,only linux AMI</td>
            </tr>
            <tr>
              <td>EBS</td>
              <td>when data needs to be persisted between terminations, faster io , application storage and DB, </td>
              <td>less costly than EFS</td>
              <td>not HA,can only attach to one instance</td>
            </tr>
            <tr>
              <td>AWS Backup</td>
              <td>centrally manage and automate backup of EC2/S3/RDS/EFS/DocumentDB/AWSstoragegateway,cross region and
                cross account backup,support PITR </td>
              <td></td>
              <td>Fully managed service, automatically internally backedup in S3,WORM support</td>
            </tr>
            <tr>
              <td>AWS DyanamoDB</td>
              <td>Mobile gaming ,sensor network, gaming, digital ad serving, ecommerce shopping.,
                web session management,capturing live events like voting,meta data storage for s3,
                any time data is hot and small and to be injested at scale
                When not to use : Relational data,blob,joins, low read writes
              </td>
              <td>normal pay by provision,ondemand is 2.5 times expensive</td>
              <td>Fully managed service, High HA multi region </td>
            </tr>
            <tr>
              <td>AWS RDS</td>
              <td>rdbms like aurora,mysql,mariadb,posgressql,oracle,sql server: for small data,OLTP</td>
              <td></td>
              <td></td>
            </tr>
            <tr>
              <td>AWS DocumentDB</td>
              <td>mongodb replacementJSON ,Million req/s,increments of 10Gb, web mobile games,streaming,OLTP</td>
              <td></td>
              <td>Fully managed service, High HA multi region 3AZ</td>
            </tr>
            <tr>
              <td>AWS MemmoryDB</td>
              <td>Redis compatible API in memmory Database 160M req/s,10GB,100TB,ultra fast</td>
              <td></td>
              <td>Multi az transaction log</td>
            </tr>

            <tr>
              <td>AWS Keyspaces</td>
              <td>Apache cassandra,nosql distributed,fully managed,iot devices, time-series,cql </td>
              <td>fully managed,</td>
              <td>single digit ms latency, 1000Req/s,multi az 3 times replication</td>
            </tr>
            <tr>
              <td>AWS Neptune</td>
              <td>fully managed graph DB, social network, knowldge graph/wiki,fraud detection,
                recommendation engine, QUERY?: Gremlin/SparkQL/openCypher</td>
              <td></td>
              <td>multi az 3 times replication</td>
            </tr>
            <tr>
              <td>AWS Timestream</td>
              <td>Time series, serverless , Trillion events /day,sql compatible
                IOT apps, operational apps,real time analytics
              </td>
              <td></td>
              <td></td>
            </tr>
            <tr>
              <td>AWS Redshift</td>
              <td>fully managed petabyte scale,data warehouse,MPP,OLAP,analytics,ad impression clicks,
                social trends,global sales data,historical stock data ,DW modernization,gaming data aggregation</td>
              <td></td>
              <td></td>
            </tr>
            <tr>
              <td>AWS Data sync</td>
              <td>DataSync is best for transferring data between on-premises and AWS, NFS/SMB(onprem) to AWS
                (s3/EFS/Fsx/...) </td>
              <td> </td>
              <td></td>
            </tr>
            <tr>
              <td>AWS transfer family</td>
              <td> sftp/ftp/ftps to s3 </td>
              <td>provisioned endpoint /hr + data transfer</td>
              <td></td>
            </tr>
            <tr>
              <td>AWS EC2</td>
              <td>Spot instances are good candidate for ML /big data work with checkpoint enabled.so things can be
                restored</td>
              <td></td>
              <td></td>
            </tr>
            <tr>
              <td>AWS Lambda</td>
              <td> run code snippets, stateless bit of code proceessing ,it can act as a glue between services
                Lamda polls KDS to pull data </td>
              <td></td>
              <td></td>
            </tr>
            <tr>
              <td>AWS SAM</td>
              <td> Serverless Application Model, is a framework for developing and deploying serverless applications,
                using YAML code ,generates cloudformation scripts </td>
              <td></td>
              <td></td>
            </tr>
            <tr>
              <td>AWS Batch</td>
              <td> good for non ETL work ,
                runs batch jobs (cleaning fs, processing images basically any computing job )on Docker images ,
                serverless , you can schdeule by cloudwatch events or via step function ,
                is designed specifically for high-throughput batch computing workloads. </td>
              <td>pay for ec2 instances </td>
              <td></td>
            </tr>
            <tr>
              <td>AWS Graviton</td>
              <td> compute memmory optimized good for android game streaming </td>
              <td> good price performance ratio</td>
              <td></td>
            </tr>
            <tr>
              <td>AWS ECS</td>
              <td> AWS ECS as a) EC2 launch type needs ECS agent on ec2 b) Fargate lauch type(serverless)
                s3 cannot be mounted as a filesystem on ECS only EFS
              </td>
              <td> </td>
              <td></td>
            </tr>
            <tr>
              <td>AWS ECR </td>
              <td>Elastic Container Registry:Store and manage docker images from public and private repo
              </td>
              <td> </td>
              <td></td>
            </tr>
            <tr>
              <td>AWS EKS </td>
              <td>Elastic Kubernetes Service, managed K8s cluster in AWS ,K8s is cloud agnostic
                a) Managed b) Self Managed Node groups c) Fargate
                EBS,EFS(for Fargate),FSx support Netapp/Lustre
              </td>
              <td> </td>
              <td></td>
            </tr>
            <tr>
              <td>AWS Glue </td>
              <td>Any ETL job(data side),fully managed,
                servlerless discovery and table defination/schema and publish to athena,redshift,sql db's from
                datalakes,rds,redshift
                a) periodically and ondemand,data remains in source place.
                Glue crawler extracts data in partitions from s3 and puts in Glue Data catalog
                Hive: run SQL like queries from EMR, GLUE <-> hive
                  Glue ETL : can be event driven (triggers)or schdeule,Encryption,DPU by running job metrics in Glue
                  Console.
                  supports bookmarks
                  Glue for Spark , for other engines like hive pig use data pipeline EMR
                  Spark Structured Streaming is supported from Kinesis /Kafka
                  AWS Glue studio: Visual job editor DAG,visual job dashboard
                  AWS Glue Data Quality: DQDL fail or report to cloud watch
                  AWS Glue Data Brew: visual data preperation tool,output to S3 ,clean and normalize data
                  AWS Glue Workflows:viual orchestration tool for glue for multi job multi crawler, Triggers schdeuled
                  or on demand or event bridge
              </td>
              <td>cost effective,billed by sec for crawler, billed by min for development ,first million obj are free
                for Glue data catalog </td>
              <td></td>
            </tr>
            <tr>
              <td>AWS Lake formation </td>
              <td> Build on top of glue, creates secure data lakes in days,auditing,access control,encryption,anything
                glue can do it can do
                then use athena/redshift/emr to query
                Access can be given to only administrator reciepent and AWS Resouce Access manager for external accounts
                and
                IAM permissions for cross account access
                Governed Tables: provides ACID support automatic  optimization/comaction is done
                Row coloumn cell level security
                use case: good for column based security
              </td>
              <td>no cost ,cost of unerlying sources and glue/s3/emr/redshift </td>
              <td></td>
            </tr>
            <tr>
              <td>AWS Athena </td>
              <td> Serverless interactive query s3 or data lake, data stays in s3 ,
                Athena Workgroups: Organize teams/users/apps via WORKGROUPS individual limit can be set
                Encryption:SSE-S3,SSE-KMS,CSE-KMS at rest and TLS on transit
                Do not use for formatted visualization use quicksight , do not use for ETL use Glue,
                small number of large files perform better than large number of small files, ORC Parquet better (splittable/coloumnar)
                ACID transaction support by Table type iceberg, but manual optimization/comaction is needed
                IAM based Operation based security unlike Data Lake
              </td>
              <td>pay as u go,failed and DDL are not charged </td>
              <td></td>
            </tr>
            <tr>
              <td>AWS Apache spark </td>
              <td> Usecase: distributed processing for big data for OLAP , batch,interactive,graph and stream processing ,machine learning mllib
                spark is not meant for OLTP 
                Amazon athena can work with apache spark by selcting spark engine insted of SQL engine
              </td>
              <td> </td>
              <td></td>
            </tr>
            <tr>
              <td>AWS Athena federated query </td>
              <td>  Query data other than s3, supports athena views,athena views are stored in glue not in DB.
                Can do cross account federated query,and passthrough query for native query
              </td>
              <td> </td>
              <td></td>
            </tr>
            <tr>
              <td>AWS EMR </td>
              <td> is a managed hadoop instances on ec2,master(manages),core(store data),task(processing) nodes ,
                 use spot instance for task node
                 Transient vs Long running clusters
                 supports s3 for input and output data, HDFS is ephipermal EBS,EMRFS
              </td>
              <td> charge by hr</td>
              <td> Managed Scaling , ADD core node then task node, REMOVE task node then core node</td>
            </tr>
            <tr>
              <td>AWS EMR serverless </td>
              <td> no longer need to estimate capacity, but add 10% more for initial capacity request
                EMR on EKS  
              </td>
              <td></td>
              <td>all within one region across multiple AZ</td>
            </tr>

            <tr>
              <td>AWS Kiniseis data stream </td>
              <td> data is distrubited in shards ,shard need to be provisioned during creation
                Retention : 1-365 days ,immutablity data
               Producer : Partition Key + 1MB data blob, so 1MB/s or 1000msg/s
               Consumer : Partition Key + seq+blob , Shared : 2Mb/s per shard all consumers or Enhanced: 2MB/s per shard/conumser
               Kinesis Producer SDK : usecase low throughput , high latency,AWS lambda,simple api , use for latest record like iot
               ProvsionedThroughputExceededException : Hot Partionkey problem do retry backoff.increase shard,distribute key
               Kinesis Producer Librabry : C++ /Java supports Synchronous and Asynchronous api , 
               Batching (Collect & aggregate) RecordMaxBufferedTime 100ms
               KCL : ExpiredIteraetor Exception : increase WCU in Dynamodb, KCL supports checkpointing Consumer Librabry but is polling
               KConnector lirary : needs EC2 runs as agent and pushes to s3/redhift,dynamodb/opensearch
               KCL Enhanced fanout :is pushing data , 2mb/s per shard/consumer,use case : low latency req ~70ms. many consumers
               Standardard consumers: usecase less conusmers ,tolerate 200ms latency,low cost
               Kiniesis Shard splitting (handle high traffic) and merge shard (handle low traffic and cost)
               Out of order records after resharding : make sure consumer reads from parent first entierly until no records before going to splitted shard
              </td>
              <td>Provioned mode: pay per shard /hr  
                  On demand mode: pay per shard /hr  & data in out /GB 
              </td>
              <td>KCL manual scaling no auto</td>
            </tr>
            <tr>
              <td>AWS Kinesis Datafirehose </td>
              <td> near realtime in batches , buffer time or space, 4 places to send data Opensearch,s3,redshift,splunk
                transform via lambda
              </td>
              <td> streams has custome code for consumer and producer , Datafirehose is fully managed no data storage </td>
              <td></td>
            </tr>
            <tr>
              <td>AWS MSK </td>
              <td> use when data storgae needs is higher 1MB by default but is configurable, Topics with partitions
                you can add partion not remove
              </td>
              <td> </td>
              <td></td>
            </tr>
            <tr>
              <td>AWS quicksight </td>
              <td> formatted customized visualization business analytics
                USECASE: adhoc exploration and visualization of data
                Data sources :redshift,athena,s3,aurora, rds, opensearch,iot analytics,files csv/tsv/log formats,SAAS salesforce
                SPICE : Super fast parallel in memory calculation engine
                Dont use ETL only limited transformation
                RLS and CLS row /column level security
                create new security groups with inbound rules from the ipranges of quick sight to redshift in that region
                for Enterprise edition use ENI , 
                perering connection  , transit gateway or vpc sharing  for crossaccount access
                Quicksight Q: NLP support 
                ML insights,forcasting,Anomaly detection,Auto narratives
                calculated Fields
                Level aware calculations aggergation or window function
                Fixed Precision: 4 decimnal 
                FLoat Precision: 16 decimal 
              </td>
              <td> </td>
              <td></td>
            </tr>
            <tr>
              <td>AWS opensearch and serverless </td>
              <td> anlaysis search reporting petabyte scaling
                text search, log analytics, app monitoring, secuity analytics
                dont use for OLTP
                serverless: on demand , collections, encryption at rest ,OCU default 2 units r/w
              </td>
              <td> </td>
              <td></td>
            </tr>
            <tr>
              <td>AWS Datapipeline </td>
              <td> to schedule jobs
              </td>
              <td> </td>
              <td></td>
            </tr>
          </tbody>
        </table>
        <br>
        <p>
          Data Mesh:
          Each department/team is responsible for own data ,
          these teams own the data products,
          These data products serve different uses cases across organization,
          Federation there is a centralized way how data is accessed adhering to organizational principle.
          Data Management Paradigm: Domain based data management
          Self service tooling and infrastructure.
        </p>
        <p>Data sources: JDBC,ODBC,Streams,logs,api's</p>
        <p>Data formats: csv,json,avro(binary,schema+data),parquet(coloumn based,optimized for
          anlytics,compression,disttributed systems)</p>
        <p>AWS EC2</p>

        <p><img src="/docs/assets/images/AWS-EC2.png" alt="AWS EC2" /></p>

        <p>AWS MWAA Airflow
          <img src="/docs/assets/images/AWS-Airflow.png" alt="AWS MWAA" />
        </p>

        <p>AWS DataSync</p>

        <p><img src="/docs/assets/images/AWS-DataSync.svg" alt="AWS DataSync" /></p>

      </section>

      <aside id="sidebar">
        <p>Quicklinks
          <a href="https://bipulkkuri.github.io/AWS-DEA-C01/mindmap/" class="button"> Mindmap</a>
        </p>
      </aside>
    </div>
  </div>

</body>

</html>
